{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dKPGRLe0JCCB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vr.programs_math\n",
    "from vr.preprocess import tokenize, encode, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_to_str(program, mode):\n",
    "    if mode == 'chain':\n",
    "        if not vr.programs_math.is_chain(program):\n",
    "            return None\n",
    "        return vr.programs_math.list_to_str(program)\n",
    "    elif mode == 'prefix':\n",
    "        program_prefix = vr.programs_math.list_to_prefix(program)\n",
    "        return vr.programs_math.list_to_str(program_prefix)\n",
    "    elif mode == 'postfix':\n",
    "        program_postfix = vr.programs_math.list_to_postfix(program)\n",
    "        return vr.programs_math.list_to_str(program_postfix)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(output_h5_file, questions, vocab):\n",
    "    print('Encoding data')\n",
    "    questions_encoded = []\n",
    "    programs_encoded = []\n",
    "    question_families = []\n",
    "    orig_idxs = []\n",
    "    image_idxs = []\n",
    "    answers = []\n",
    "    types = []\n",
    "\n",
    "    for orig_idx, q in enumerate(questions):\n",
    "        question = q['question']\n",
    "        if 'program' in q:\n",
    "            types += [q['program'][-1]['type']] # takes last program for a given input and the type of program\n",
    "        orig_idxs.append(orig_idx)\n",
    "        image_idxs.append(q['image_index'])\n",
    "        if 'question_family_index' in q:\n",
    "            question_families.append(q['question_family_index'])\n",
    "        question_tokens = tokenize(question,\n",
    "                            punct_to_keep=[';', ','],\n",
    "                            punct_to_remove=['?', '.'])\n",
    "        question_encoded = encode(question_tokens,\n",
    "                             vocab['question_token_to_idx'],\n",
    "                             allow_unk=encode_unk == 1)\n",
    "        questions_encoded.append(question_encoded)\n",
    "        if 'program' in q:\n",
    "            program = q['program']\n",
    "            program_str = program_to_str(program, mode)\n",
    "            program_tokens = tokenize(program_str)\n",
    "            program_encoded = encode(program_tokens, vocab['program_token_to_idx'])\n",
    "            programs_encoded.append(program_encoded)\n",
    "        if 'answer' in q:\n",
    "            answers.append(vocab['answer_token_to_idx'][str(q['answer'])])\n",
    "\n",
    "    # Pad encoded questions and programs\n",
    "    max_question_length = max(len(x) for x in questions_encoded)\n",
    "    for qe in questions_encoded:\n",
    "        while len(qe) < max_question_length:\n",
    "            qe.append(vocab['question_token_to_idx']['<NULL>'])\n",
    "\n",
    "    if len(programs_encoded) > 0:\n",
    "        max_program_length = max(len(x) for x in programs_encoded)\n",
    "        for pe in programs_encoded:\n",
    "            while len(pe) < max_program_length:\n",
    "                pe.append(vocab['program_token_to_idx']['<NULL>'])\n",
    "                \n",
    "    print('Writing output')\n",
    "    questions_encoded = np.asarray(questions_encoded, dtype=np.int32)\n",
    "    programs_encoded = np.asarray(programs_encoded, dtype=np.int32)\n",
    "    print(questions_encoded.shape)\n",
    "    print(programs_encoded.shape)\n",
    "    \n",
    "    mapping = {}\n",
    "    for i, t in enumerate(set(types)):\n",
    "        mapping[t] = i\n",
    "    print(mapping)\n",
    "    types_coded = []\n",
    "    for t in types:\n",
    "        types_coded += [mapping[t]]\n",
    "    \n",
    "    with h5py.File(output_h5_file, 'w') as f:\n",
    "        f.create_dataset('questions', data=questions_encoded)\n",
    "        f.create_dataset('image_idxs', data=np.asarray(image_idxs))\n",
    "        f.create_dataset('orig_idxs', data=np.asarray(orig_idxs))\n",
    "\n",
    "        if len(programs_encoded) > 0:\n",
    "            f.create_dataset('programs', data=programs_encoded)\n",
    "        if len(question_families) > 0:\n",
    "            f.create_dataset('question_families', data=np.asarray(question_families))\n",
    "        if len(answers) > 0:\n",
    "            f.create_dataset('answers', data=np.asarray(answers))\n",
    "        if len(types) > 0:\n",
    "            f.create_dataset('types', data=np.asarray(types_coded))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(output_h5_file, input_questions_json, input_vocab_json='', output_vocab_json=''):\n",
    "    \n",
    "    if (input_vocab_json == '') and (output_vocab_json == ''):\n",
    "        print('Must give one of --input_vocab_json or --output_vocab_json')\n",
    "        return\n",
    "    \n",
    "    print('Loading data')\n",
    "    with open(input_questions_json, 'r') as f:\n",
    "        questions = json.load(f)['questions']\n",
    "        \n",
    "    if input_vocab_json == '' or expand_vocab == 1:\n",
    "        print('Building vocab')\n",
    "        if 'answer' in questions[0]:\n",
    "            answer_token_to_idx = build_vocab(\n",
    "                (str(q['answer']) for q in questions)\n",
    "            )\n",
    "        question_token_to_idx = build_vocab(\n",
    "            (q['question'] for q in questions),\n",
    "            min_token_count=unk_threshold,\n",
    "            punct_to_keep=[';', ','], punct_to_remove=['?', '.']\n",
    "        )\n",
    "        all_program_strs = []\n",
    "        for q in questions:\n",
    "            if 'program' not in q: continue\n",
    "            program_str = program_to_str(q['program'], mode)\n",
    "            if program_str is not None:\n",
    "                all_program_strs.append(program_str)\n",
    "        program_token_to_idx = build_vocab(all_program_strs)\n",
    "        vocab = {\n",
    "            'question_token_to_idx': question_token_to_idx,\n",
    "            'program_token_to_idx': program_token_to_idx,\n",
    "            'answer_token_to_idx': answer_token_to_idx,\n",
    "        }\n",
    "    \n",
    "    if input_vocab_json != '':\n",
    "        print('Loading vocab')\n",
    "        if expand_vocab == 1:\n",
    "            new_vocab = vocab\n",
    "        with open(input_vocab_json, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        if expand_vocab == 1:\n",
    "            num_new_words = 0\n",
    "            for word in new_vocab['question_token_to_idx']:\n",
    "                if word not in vocab['question_token_to_idx']:\n",
    "                    print('Found new word %s' % word)\n",
    "                    idx = len(vocab['question_token_to_idx'])\n",
    "                    vocab['question_token_to_idx'][word] = idx\n",
    "                    num_new_words += 1\n",
    "            print('Found %d new words' % num_new_words)\n",
    "\n",
    "    \n",
    "    if output_vocab_json != '':\n",
    "        with open(output_vocab_json, 'w') as f:\n",
    "            json.dump(vocab, f)\n",
    "            \n",
    "    encode_data(output_h5_file, questions, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================== Args =========================================================**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'prefix'\n",
    "expand_vocab = 0\n",
    "unk_threshold = 1\n",
    "encode_unk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1HOeL1jyJWNr"
   },
   "outputs": [],
   "source": [
    "train_questions_path = 'data/CLEVR_vMath/questions/clevr-math-train.json'\n",
    "valid_questions_path = 'data/CLEVR_vMath/questions/clevr-math-val.json'\n",
    "test_questions_path = 'data/CLEVR_vMath/questions/clevr-math-test.json'\n",
    "\n",
    "train_output_h5_file = 'data/train_questions_math.h5'\n",
    "val_output_h5_file = 'data/val_questions_math.h5'\n",
    "test_output_h5_file = 'data/test_questions_math.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3BiiV2hies"
   },
   "source": [
    "**====================================================== Main =========================================================**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Building vocab\n",
      "Encoding data\n",
      "Writing output\n",
      "(556082, 19)\n",
      "(556082, 18)\n",
      "{'count': 0, 'addition': 1, 'subtraction': 2}\n",
      "Loading data\n",
      "Loading vocab\n",
      "Encoding data\n",
      "Writing output\n",
      "(119202, 19)\n",
      "(119202, 18)\n",
      "{'count': 0, 'addition': 1, 'subtraction': 2}\n",
      "Loading data\n",
      "Loading vocab\n",
      "Encoding data\n",
      "Writing output\n",
      "(7955, 19)\n",
      "(7955, 18)\n",
      "{'count': 0, 'addition': 1, 'subtraction': 2}\n"
     ]
    }
   ],
   "source": [
    "preprocess(train_output_h5_file, train_questions_path, output_vocab_json='data/vocab_math.json')\n",
    "preprocess(val_output_h5_file, valid_questions_path, input_vocab_json='data/vocab_math.json')\n",
    "preprocess(test_output_h5_file, test_questions_path, input_vocab_json='data/vocab_math.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding Programs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtract all blue balls. Subtract all gray blocks. How many balls are left?\n",
      "[{'type': 'scene', 'inputs': [], '_output': [0, 1, 2, 3, 4], 'value_inputs': []}, {'type': 'filter_shape', 'inputs': [0], '_output': [3], 'value_inputs': ['sphere']}, {'type': 'filter_color', 'inputs': [1], '_output': [], 'value_inputs': ['blue']}, {'type': 'count', 'inputs': [1], '_output': 1, 'value_inputs': []}, {'type': 'count', 'inputs': [2], '_output': 0, 'value_inputs': []}, {'type': 'filter_shape', 'inputs': [0], '_output': [1, 4], 'value_inputs': ['cube']}, {'type': 'filter_color', 'inputs': [5], '_output': [1, 4], 'value_inputs': ['gray']}, {'type': 'subtraction', 'inputs': [3, 4], '_output': 1, 'value_inputs': []}]\n",
      "subtraction count filter_shape['sphere'] scene count filter_color['blue'] filter_shape['sphere'] scene\n",
      "['<START>', 'subtraction', 'count', \"filter_shape['sphere']\", 'scene', 'count', \"filter_color['blue']\", \"filter_shape['sphere']\", 'scene', '<END>']\n",
      "[1, 47, 20, 42, 46, 20, 21, 42, 46, 2]\n",
      "\n",
      "[1]\n",
      "Subtract all gray cubes. How many red cylinders are left?\n",
      "[{'type': 'scene', 'inputs': [], '_output': [0, 1, 2, 3, 4], 'value_inputs': []}, {'type': 'filter_shape', 'inputs': [0], '_output': [1, 4], 'value_inputs': ['cube']}, {'type': 'filter_color', 'inputs': [1], '_output': [1, 4], 'value_inputs': ['gray']}, {'type': 'filter_shape', 'inputs': [0], '_output': [0, 2], 'value_inputs': ['cylinder']}, {'type': 'filter_color', 'inputs': [3], '_output': [], 'value_inputs': ['red']}, {'type': 'count', 'inputs': [4], '_output': 0, 'value_inputs': []}]\n",
      "count filter_color['red'] filter_shape['cylinder'] scene\n",
      "['<START>', 'count', \"filter_color['red']\", \"filter_shape['cylinder']\", 'scene', '<END>']\n",
      "[1, 20, 28, 37, 46, 2]\n",
      "\n",
      "[0]\n",
      "Add 1 small objects. How many greens exist?\n",
      "[{'type': 'scene', 'inputs': [], '_output': [0, 1, 2, 3, 4], 'value_inputs': []}, {'type': 'filter_color', 'inputs': [0], '_output': [], 'value_inputs': ['small']}, {'type': 'filter_shape', 'inputs': [0], '_output': [], 'value_inputs': ['green']}, {'type': 'choose', 'inputs': [0], '_output': 1, 'value_inputs': ['1']}, {'type': 'count', 'inputs': [2], '_output': 0, 'value_inputs': []}]\n",
      "count filter_shape['green'] scene\n",
      "['<START>', 'count', \"filter_shape['green']\", 'scene', '<END>']\n",
      "[1, 20, 39, 46, 2]\n",
      "\n",
      "[0]\n",
      "Subtract all tiny matte blocks. Subtract all big purple things. How many objects are left?\n",
      "[{'type': 'scene', 'inputs': [], '_output': [0, 1, 2, 3, 4], 'value_inputs': []}, {'type': 'filter_size', 'inputs': [0], '_output': [2, 4], 'value_inputs': ['small']}, {'type': 'filter_material', 'inputs': [1], '_output': [2], 'value_inputs': ['rubber']}, {'type': 'filter_shape', 'inputs': [2], '_output': [], 'value_inputs': ['cube']}, {'type': 'filter_size', 'inputs': [0], '_output': [0, 1, 3], 'value_inputs': ['large']}, {'type': 'filter_color', 'inputs': [4], '_output': [3], 'value_inputs': ['purple']}, {'type': 'count', 'inputs': [0], '_output': 5, 'value_inputs': []}, {'type': 'count', 'inputs': [3], '_output': 0, 'value_inputs': []}, {'type': 'count', 'inputs': [5], '_output': 1, 'value_inputs': []}, {'type': 'subtraction', 'inputs': [6, 7], '_output': 5, 'value_inputs': []}, {'type': 'subtraction', 'inputs': [9, 8], '_output': 4, 'value_inputs': []}]\n",
      "subtraction subtraction count scene count filter_shape['cube'] filter_material['rubber'] filter_size['small'] scene count filter_color['purple'] filter_size['large'] scene\n",
      "['<START>', 'subtraction', 'subtraction', 'count', 'scene', 'count', \"filter_shape['cube']\", \"filter_material['rubber']\", \"filter_size['small']\", 'scene', 'count', \"filter_color['purple']\", \"filter_size['large']\", 'scene', '<END>']\n",
      "[1, 47, 47, 20, 46, 20, 35, 32, 45, 46, 20, 27, 44, 46, 2]\n",
      "\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "with open('data/vocab_math.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "with open('data/CLEVR_vMath/questions/clevr-math-val.json', 'r') as f:\n",
    "    questions = json.load(f)['questions']\n",
    "    for i, q in enumerate(questions):\n",
    "        if 'program' not in q: continue\n",
    "        print(q['question'])\n",
    "        print(q['program'])\n",
    "        program_str = program_to_str(q['program'], mode)\n",
    "        if program_str is not None:\n",
    "            print(program_str)\n",
    "        program_tokens = tokenize(program_str)\n",
    "        print(program_tokens)\n",
    "        program_encoded = encode(program_tokens, vocab['program_token_to_idx'])\n",
    "        print(program_encoded)\n",
    "        print()\n",
    "        if 'answer' in q:\n",
    "            print([q['answer']])\n",
    "        if i == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
